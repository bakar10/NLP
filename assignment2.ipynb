{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "id": "-zbd78VlMmXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "vMvhI2juMs7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9xknFDvLlLk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import math\n",
        "from math import log\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize NLTK's Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Initialize the text generation pipeline with the desired model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Define the prompts\n",
        "topics = [\n",
        "    \"machine learning\",\n",
        "    \"Lionel Messi\"\n",
        "]\n",
        "\n",
        "# List to store generated documents\n",
        "generated_documents = []\n",
        "\n",
        "# Generate text based on each topic and store the documents\n",
        "for i, topic in enumerate(topics, start=1):\n",
        "    generated_text = generator(topic, max_length=50)\n",
        "    generated_documents.append(generated_text[0]['generated_text'])\n",
        "\n",
        "# Print the generated documents\n",
        "print(\"Generated Documents:\")\n",
        "for i, doc in enumerate(generated_documents, start=1):\n",
        "    print(\"Document\", i, \":\", doc)\n",
        "    print()\n",
        "\n",
        "# Initialize NLTK's stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and normalize the text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove symbols and characters\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split the text into words\n",
        "    words = cleaned_text.split()\n",
        "    # Remove stop words and stem each word\n",
        "    filtered_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "# List to store unique words for each document\n",
        "preprocessed_documents = []\n",
        "\n",
        "# Get unique words from each document separately\n",
        "for doc in generated_documents:\n",
        "    preprocessed_doc = preprocess_text(doc)\n",
        "    unique_words = set(preprocessed_doc)\n",
        "    preprocessed_documents.append(unique_words)\n",
        "\n",
        "# Print the unique words for each document separately\n",
        "for i, unique_words in enumerate(preprocessed_documents, start=1):\n",
        "    print(\"Unique words in Document\", i, \":\")\n",
        "    for word in unique_words:\n",
        "        print(word)\n",
        "    print()\n",
        "\n",
        "# Compute TF\n",
        "def compute_tf(document):\n",
        "    tf = Counter(document)\n",
        "    for word in tf:\n",
        "        tf[word] = tf[word] / len(document)\n",
        "    return tf\n",
        "\n",
        "# Compute IDF\n",
        "def compute_idf(documents):\n",
        "    idf = {}\n",
        "    total_docs = len(documents)\n",
        "    words_in_documents = [set(document) for document in documents]\n",
        "    all_words = set().union(*words_in_documents)\n",
        "    for word in all_words:\n",
        "        doc_count = sum([1 for document in documents if word in document])\n",
        "        idf[word] = log(total_docs / (doc_count + 1))\n",
        "    return idf\n",
        "\n",
        "# Compute TF-IDF\n",
        "def compute_tfidf(documents):\n",
        "    tfidf = []\n",
        "    idf = compute_idf(documents)\n",
        "    for document in documents:\n",
        "        tf = compute_tf(document)\n",
        "        tfidf_doc = {word: tf[word] * idf[word] for word in tf}\n",
        "        tfidf.append(tfidf_doc)\n",
        "    return tfidf\n",
        "\n",
        "# Compute TF-IDF from scratch\n",
        "tfidf_scratch = compute_tfidf(preprocessed_documents)\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [' '.join(doc) for doc in preprocessed_documents]\n",
        "\n",
        "# Compute TF-IDF using scikit-learn\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF-IDF using scikit-learn\n",
        "print(\"TF-IDF using scikit-learn:\")\n",
        "for i in range(len(preprocessed_documents)):\n",
        "    print(\"Document\", i+1, \":\")\n",
        "    for j, word in enumerate(feature_names):\n",
        "        tfidf_value = tfidf_matrix[i, j]\n",
        "        if tfidf_value != 0:\n",
        "            print(word, \":\", tfidf_value)\n",
        "    print()"
      ]
    }
  ]
}