{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmBdAZdW-E4J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h7eK4ynKwKme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb679d7-c006-4ca0-9987-a8e028fcc8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment         ids                          date      flag  \\\n",
            "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "\n",
            "              user                                               text  \n",
            "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your dataset\n",
        "file_path = 'training.1600000.processed.noemoticon.csv'\n",
        "import dask.dataframe as dd\n",
        "\n",
        "\n",
        "# List of encodings to try\n",
        "encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1', 'utf-16']\n",
        "column_names = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "\n",
        "# Iterate over each encoding and try reading the file\n",
        "for encoding in encodings_to_try:\n",
        "    try:\n",
        "\n",
        "        # Read the dataset using Dask with the current encoding\n",
        "        df = dd.read_csv(file_path, encoding=encoding,names=column_names,blocksize=None)\n",
        "\n",
        "        # Compute and display the first few rows\n",
        "        print(df.head())\n",
        "\n",
        "        # If no exception is raised, break the loop\n",
        "        break\n",
        "    except UnicodeDecodeError:\n",
        "        # If an exception is raised, continue to the next encoding\n",
        "        continue\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_gUE1m-4JaK",
        "outputId": "d0c09014-01c7-44e9-9217-155078988467"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         sentiment         ids                          date      flag  \\\n",
            "1002697          4  1960186342  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
            "1002698          4  1960186409  Fri May 29 07:33:43 PDT 2009  NO_QUERY   \n",
            "1002699          4  1960186429  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
            "1002700          4  1960186445  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
            "1002701          4  1960186607  Fri May 29 07:33:45 PDT 2009  NO_QUERY   \n",
            "\n",
            "                    user                                               text  \n",
            "1002697  Madelinedugganx           My GrandMa is making Dinenr with my Mum   \n",
            "1002698     OffRoad_Dude  Mid-morning snack time... A bowl of cheese noo...  \n",
            "1002699         Falchion  @ShaDeLa same here  say it like from the Termi...  \n",
            "1002700   jonasobsessedx             @DestinyHope92 im great thaanks  wbuu?  \n",
            "1002701        sugababez               cant wait til her date this weekend   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "X = df[\"text\"]\n",
        "y = df[\"sentiment\"]\n",
        "# Tokenize the text data\n",
        "max_words = 10000  # Adjust this value as needed\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "PHoQZCnLiDZj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "TLjbO6Ioidh9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = 100  # Adjust this value as needed\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_sequence_length)"
      ],
      "metadata": {
        "id": "QL7wQgorkuZ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KGw4THCWikYp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "embedding_dim = 100  # Adjust this value as needed\n",
        "num_filters = 128  # Adjust this value as needed\n",
        "kernel_size = 5\n",
        "dropout_rate = 0.5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=100))\n",
        "model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "pbOOpKlxixxb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 5  # Adjust this value as needed\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "GydKt972kPrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e658033-0331-40f8-fa7e-951148b65834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10027/10027 [==============================] - 83s 8ms/step - loss: 0.3587 - accuracy: 0.8463 - val_loss: 0.3337 - val_accuracy: 0.8572\n",
            "Epoch 2/5\n",
            "10027/10027 [==============================] - 51s 5ms/step - loss: 0.3133 - accuracy: 0.8679 - val_loss: 0.3319 - val_accuracy: 0.8592\n",
            "Epoch 3/5\n",
            "10027/10027 [==============================] - 55s 6ms/step - loss: 0.2790 - accuracy: 0.8839 - val_loss: 0.3418 - val_accuracy: 0.8571\n",
            "Epoch 4/5\n",
            "10027/10027 [==============================] - 50s 5ms/step - loss: 0.2411 - accuracy: 0.9008 - val_loss: 0.3689 - val_accuracy: 0.8478\n",
            "Epoch 5/5\n",
            "10027/10027 [==============================] - 54s 5ms/step - loss: 0.2045 - accuracy: 0.9166 - val_loss: 0.4223 - val_accuracy: 0.8438\n",
            "6267/6267 [==============================] - 16s 3ms/step - loss: 0.4205 - accuracy: 0.8446\n",
            "Test Loss: 0.42047885060310364\n",
            "Test Accuracy: 0.8445854187011719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simpler CNN model with reduced complexity\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))  # Reduced number of filters\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))  # Reduced number of neurons\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the simplified model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the simplified model\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the simplified model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sonuzSOLnd2Q",
        "outputId": "ab86bad3-b4cc-4691-f575-5e687a95b8f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10027/10027 [==============================] - 65s 6ms/step - loss: 0.3525 - accuracy: 0.8474 - val_loss: 0.3361 - val_accuracy: 0.8551\n",
            "Epoch 2/5\n",
            "10027/10027 [==============================] - 49s 5ms/step - loss: 0.3058 - accuracy: 0.8708 - val_loss: 0.3326 - val_accuracy: 0.8587\n",
            "Epoch 3/5\n",
            "10027/10027 [==============================] - 48s 5ms/step - loss: 0.2724 - accuracy: 0.8872 - val_loss: 0.3391 - val_accuracy: 0.8561\n",
            "Epoch 4/5\n",
            "10027/10027 [==============================] - 47s 5ms/step - loss: 0.2394 - accuracy: 0.9025 - val_loss: 0.3543 - val_accuracy: 0.8528\n",
            "Epoch 5/5\n",
            "10027/10027 [==============================] - 53s 5ms/step - loss: 0.2076 - accuracy: 0.9171 - val_loss: 0.4065 - val_accuracy: 0.8476\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.4017 - accuracy: 0.8494\n",
            "Test Loss: 0.4017302095890045\n",
            "Test Accuracy: 0.8493824005126953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MaxPooling1D, Flatten\n",
        "\n",
        "# Define the second CNN model architecture\n",
        "model_simple = Sequential()\n",
        "model_simple.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_simple.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_simple.add(MaxPooling1D(pool_size=2))  # Adding a MaxPooling layer\n",
        "model_simple.add(Flatten())  # Flatten the output of the convolutional layer\n",
        "model_simple.add(Dense(64, activation='relu'))\n",
        "model_simple.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the second model\n",
        "model_simple.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the second model\n",
        "history_simple = model_simple.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the second model on the test set\n",
        "loss_simple, accuracy_simple = model_simple.evaluate(X_test, y_test)\n",
        "print(\"Test Loss (Simple Model):\", loss_simple)\n",
        "print(\"Test Accuracy (Simple Model):\", accuracy_simple)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXOUgYyyJY33",
        "outputId": "9aebfaab-27e3-46be-e0e7-f9e3b89fd174"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10027/10027 [==============================] - 65s 6ms/step - loss: 0.3569 - accuracy: 0.8462 - val_loss: 0.3329 - val_accuracy: 0.8574\n",
            "Epoch 2/5\n",
            "10027/10027 [==============================] - 48s 5ms/step - loss: 0.3090 - accuracy: 0.8694 - val_loss: 0.3299 - val_accuracy: 0.8591\n",
            "Epoch 3/5\n",
            "10027/10027 [==============================] - 49s 5ms/step - loss: 0.2769 - accuracy: 0.8844 - val_loss: 0.3382 - val_accuracy: 0.8572\n",
            "Epoch 4/5\n",
            "10027/10027 [==============================] - 47s 5ms/step - loss: 0.2432 - accuracy: 0.9001 - val_loss: 0.3628 - val_accuracy: 0.8537\n",
            "Epoch 5/5\n",
            "10027/10027 [==============================] - 48s 5ms/step - loss: 0.2097 - accuracy: 0.9154 - val_loss: 0.4112 - val_accuracy: 0.8459\n",
            "6267/6267 [==============================] - 15s 2ms/step - loss: 0.4123 - accuracy: 0.8465\n",
            "Test Loss (Simple Model): 0.41225582361221313\n",
            "Test Accuracy (Simple Model): 0.8464603424072266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the third CNN model architecture\n",
        "model_complex = Sequential()\n",
        "model_complex.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model_complex.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_complex.add(MaxPooling1D(pool_size=2))\n",
        "model_complex.add(Conv1D(filters=128, kernel_size=3, activation='relu'))  # Adding another convolutional layer\n",
        "model_complex.add(MaxPooling1D(pool_size=2))\n",
        "model_complex.add(Conv1D(filters=256, kernel_size=3, activation='relu'))  # Adding another convolutional layer\n",
        "model_complex.add(GlobalMaxPooling1D())  # Using GlobalMaxPooling instead of Flatten\n",
        "model_complex.add(Dense(128, activation='relu'))\n",
        "model_complex.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the third model\n",
        "model_complex.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the third model\n",
        "history_complex = model_complex.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the third model on the test set\n",
        "loss_complex, accuracy_complex = model_complex.evaluate(X_test, y_test)\n",
        "print(\"Test Loss (Complex Model):\", loss_complex)\n",
        "print(\"Test Accuracy (Complex Model):\", accuracy_complex)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-vec0uCKjoj",
        "outputId": "8069b733-a4a3-4330-dc2e-11ba74a07291"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10027/10027 [==============================] - 81s 8ms/step - loss: 0.4157 - accuracy: 0.8123 - val_loss: 0.3967 - val_accuracy: 0.8211\n",
            "Epoch 2/5\n",
            "10027/10027 [==============================] - 58s 6ms/step - loss: 0.3779 - accuracy: 0.8318 - val_loss: 0.3915 - val_accuracy: 0.8248\n",
            "Epoch 3/5\n",
            "10027/10027 [==============================] - 58s 6ms/step - loss: 0.3516 - accuracy: 0.8452 - val_loss: 0.3992 - val_accuracy: 0.8229\n",
            "Epoch 4/5\n",
            "10027/10027 [==============================] - 62s 6ms/step - loss: 0.3236 - accuracy: 0.8595 - val_loss: 0.4070 - val_accuracy: 0.8181\n",
            "Epoch 5/5\n",
            "10027/10027 [==============================] - 62s 6ms/step - loss: 0.2949 - accuracy: 0.8734 - val_loss: 0.4408 - val_accuracy: 0.8144\n",
            "6267/6267 [==============================] - 18s 3ms/step - loss: 0.4418 - accuracy: 0.8142\n",
            "Test Loss (Complex Model): 0.44182559847831726\n",
            "Test Accuracy (Complex Model): 0.8141726851463318\n"
          ]
        }
      ]
    }
  ]
}